import os
import torch
import datasets
from datasets import load_dataset
from torch.utils.data import DataLoader, Dataset
from collections import defaultdict
import random
import json
import traceback
# ğŸš¨ PEFT ë° Hugging Face í•„ìˆ˜ import
from transformers import ( 
    AutoModel, AutoTokenizer, 
    Trainer, TrainingArguments as HfTrainingArguments,
    SchedulerType
)
from peft import LoraConfig, get_peft_model, TaskType
from sentence_transformers import SimilarityFunction
from sentence_transformers.evaluation import InformationRetrievalEvaluator
from sentence_transformers.losses import MultipleNegativesRankingLoss # Loss í•¨ìˆ˜ëŠ” ìœ ì§€


# --- 0. ì„¤ì • ë° ê²½ë¡œ ì •ì˜ ---
LOCAL_MODEL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'models', 'bge-reranker-v2-m3') 
DATA_FILE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'hsrc', 'hsrc_train.jsonl') 
OUTPUT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'models', 'bge-reranker-v2-m3_006')

# ìµœì í™” ë° LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„°
LEARNING_RATE = 2e-5      
NUM_EPOCHS = 3
WARMUP_RATIO = 0.05       
BATCH_SIZE = 8           # ğŸš¨ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ BATCH_SIZEëŠ” ë‚®ê²Œ ì„¤ì •
GRAD_ACCUM_STEPS = 4     # ğŸš¨ Gradient Accumulationì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì§ˆ ë°°ì¹˜ í¬ê¸°(32) ìœ ì§€
EVAL_STEPS = 500          
ES_PATIENCE = 3           
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
RELEVANCE_THRESHOLD = 3 

# LoRA ì„¤ì •
LORA_R = 8
LORA_ALPHA = 16


# --- 1. ë°ì´í„° ë¡œë“œ ë° í˜•ì‹ ë³€í™˜ (ë³€ê²½ ì—†ìŒ) ---
# ... (load_and_prepare_data í•¨ìˆ˜ ì½”ë“œëŠ” ë™ì¼í•˜ê²Œ ìœ ì§€) ...
def load_and_prepare_data(file_path, val_ratio=0.1, max_val_queries=2000):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Data file not found at: {file_path}")

    print(f"Loading data from: {file_path}")
    dataset = load_dataset('json', data_files={'train': file_path}, split='train')
    
    split_dataset = dataset.train_test_split(test_size=val_ratio, seed=42)
    train_set = split_dataset['train']
    val_set_full = split_dataset['test']
    val_set = val_set_full.select(range(min(len(val_set_full), max_val_queries)))
    
    # --- í›ˆë ¨ ë°ì´í„° ì¤€ë¹„: datasets.Dataset ê°ì²´ë¡œ ìœ ì§€ ---
    # PEFT TrainerëŠ” datasets.Datasetì„ ì§ì ‘ ì‚¬ìš©
    
    # í›ˆë ¨ì— í•„ìš”í•œ í•„ë“œë¥¼ í¬í•¨í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì¿¼ë¦¬, ê¸ì • ë¬¸ì„œ)
    train_data_list = []
    
    for item in train_set:
        query = item['query']
        paragraphs = item['paragraphs']
        target_actions = item['target_actions']
        
        for action_key, relevance_score_raw in target_actions.items():
            paragraph_key = action_key.replace('target_action', 'paragraph')
            
            try:
                relevance_score = int(relevance_score_raw)
            except (ValueError, TypeError):
                continue
            
            if relevance_score >= RELEVANCE_THRESHOLD and paragraph_key in paragraphs:
                positive_passage = paragraphs[paragraph_key]['passage']
                train_data_list.append({
                    'query': query,
                    'positive_passage': positive_passage
                })
    
    # ë¦¬ìŠ¤íŠ¸ë¥¼ Hugging Face Datasetìœ¼ë¡œ ë³€í™˜
    train_dataset = datasets.Dataset.from_list(train_data_list)
                
    # --- ê²€ì¦ ë°ì´í„° ì¤€ë¹„ (IR Evaluatorìš©) ---
    queries, corpus, relevant_docs = {}, {}, defaultdict(dict)

    eval_data_list = []

    # ... (ê²€ì¦ ë°ì´í„°ì…‹ êµ¬ì„± ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ...
    for idx, item in enumerate(val_set):
        query = item['query']
        paragraphs = item['paragraphs']
        target_actions = item['target_actions']
        
        q_id = item.get('query_uuid', f'q_{idx}')
        queries[q_id] = query
        
        passage_map = {} 
        for key, p_data in paragraphs.items():
            p_uuid = p_data['uuid']
            corpus[p_uuid] = p_data['passage']
            passage_map[key] = p_uuid

        for action_key, relevance_score_raw in target_actions.items():
            paragraph_key = action_key.replace('target_action', 'paragraph')
            
            try:
                relevance_score = int(relevance_score_raw)
            except (ValueError, TypeError):
                continue
            
            if paragraph_key in passage_map:
                p_uuid = passage_map[paragraph_key]
                relevant_docs[q_id][p_uuid] = relevance_score 

                # ì¶”ê°€: ê¸ì • ë¬¸ì„œ(score >= RELEVANCE_THRESHOLD)ë§Œ eval_datasetì— ì¶”ê°€
                if relevance_score >= RELEVANCE_THRESHOLD:
                    positive_passage = paragraphs[paragraph_key]['passage']
                    eval_data_list.append({
                        'query': query,
                        'positive_passage': positive_passage
                    })

    eval_dataset = datasets.Dataset.from_list(eval_data_list)

    print(f"Loaded {len(train_data_list)} (Query, Positive) pairs for training.")
    print(f"Prepared {len(queries)} validation queries.")
    print(f"Prepared {len(corpus)} documents in validation corpus (via UUIDs).")
    
    return train_dataset, eval_dataset, queries, corpus, relevant_docs


# --- 2. ì»¤ìŠ¤í…€ Trainer ë° Data Collator ì •ì˜ ---

class CustomDPRCollator:
    """í›ˆë ¨ ë°ì´í„°ë¥¼ í† í°í™”í•˜ê³  Loss ê³„ì‚°ì— í•„ìš”í•œ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, features):
        queries = [f['query'] for f in features]
        passages = [f['positive_passage'] for f in features]
        
        # ì¿¼ë¦¬ì™€ ë¬¸ë‹¨ì„ ë”°ë¡œ í† í°í™” (Bi-Encoder êµ¬ì¡°)
        q_inputs = self.tokenizer(queries, padding=True, truncation=True, return_tensors='pt')
        p_inputs = self.tokenizer(passages, padding=True, truncation=True, return_tensors='pt')
        
        return {
            'q_input_ids': q_inputs['input_ids'],
            'q_attention_mask': q_inputs['attention_mask'],
            'p_input_ids': p_inputs['input_ids'],
            'p_attention_mask': p_inputs['attention_mask'],
        }

class CustomDPRTrainer(Trainer):
    """MultipleNegativesRankingLossë¥¼ ì‚¬ìš©í•˜ì—¬ Lossë¥¼ ê³„ì‚°í•˜ëŠ” ì»¤ìŠ¤í…€ Trainer."""
    def __init__(self, model, args, data_collator, train_dataset,eval_dataset, tokenizer):
        super().__init__(
            model=model,
            args=args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset, 
            tokenizer=tokenizer,
        )

    # ğŸš¨ compute_loss ë¡œì§ì„ ì™„ì „íˆ ì¬ì‘ì„±í•˜ì—¬ S-T Lossë¥¼ ìš°íšŒ
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        
        # 1. BGE ì„ë² ë”© ìƒì„± í•¨ìˆ˜ (GPU/PEFT ëª¨ë¸ì—ì„œ ì‹¤í–‰)
        def get_embedding(input_ids, attention_mask):
            # model(AutoModel + LoRA)ì˜ ì¶œë ¥
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            
            # BGE/RoBERTaì˜ Mean Pooling êµ¬í˜„
            embeddings = outputs.last_hidden_state
            mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()
            sum_embeddings = torch.sum(embeddings * mask_expanded, 1)
            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)
            mean_embeddings = sum_embeddings / sum_mask
            
            # L2 ì •ê·œí™” (BGE í•„ìˆ˜)
            return F.normalize(mean_embeddings, p=2, dim=1)
        
        # 2. ì¿¼ë¦¬ì™€ ë¬¸ë‹¨ ì„ë² ë”© ìƒì„± (Q_emb, P_emb)
        # inputs ë”•ì…”ë„ˆë¦¬ê°€ CUDAë¡œ ì´ë™í–ˆìŒì„ ê°€ì •
        q_emb = get_embedding(inputs['q_input_ids'], inputs['q_attention_mask'])
        p_emb = get_embedding(inputs['p_input_ids'], inputs['p_attention_mask'])
        
        # 3. MNR Loss í•µì‹¬ ë¡œì§: Dot Product (ìœ ì‚¬ë„) ë° Softmax ê³„ì‚°
        # Batch Size: N
        # ì¿¼ë¦¬ ì„ë² ë”©: (N, D), ë¬¸ë‹¨ ì„ë² ë”©: (N, D)
        # ìœ ì‚¬ë„ í–‰ë ¬: (N, N)
        
        # ë‚´ì (Dot Product)ì„ í†µí•œ ìœ ì‚¬ë„ ê³„ì‚°
        # ğŸš¨ Cross-Entropy / MNR Lossì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ log_softmaxë¥¼ ì ìš©
        similarity_matrix = torch.matmul(q_emb, p_emb.transpose(0, 1)) 
        
        # 4. Loss ê³„ì‚°
        # MNR LossëŠ” In-Batch Negativesë¥¼ ì‚¬ìš©í•˜ë©°, 
        # ì •ë‹µ(Positive)ì€ ëŒ€ê°ì„ (Diagonal) ìœ„ì¹˜ì— ì¡´ì¬í•¨ (Q_i vs P_i)
        
        # Log Softmax ê³„ì‚° (Softmax(Sim)ì— Negative Log Likelihood Lossë¥¼ ì ìš©)
        log_softmax_scores = F.log_softmax(similarity_matrix, dim=1)
        
        # ì •ë‹µ ì¸ë±ìŠ¤ëŠ” [0, 1, 2, ..., N-1] ëŒ€ê°ì„ 
        target_labels = torch.arange(len(q_emb), device=q_emb.device)
        
        # Negative Log Likelihood Loss ì ìš© (ëŒ€ê°ì„  ìœ„ì¹˜ì˜ ë¡œê·¸ í™•ë¥ ì„ ìµœëŒ€í™”)
        loss = F.nll_loss(log_softmax_scores, target_labels)

        return (loss, {'loss': loss.detach()}) if return_outputs else loss

# --- 3. íŠ¸ë ˆì´ë„ˆ ì„¤ì • ë° ì‹¤í–‰ ---
def setup_lora_trainer(train_dataset, eval_dataset, queries, corpus, relevant_docs, model_path):
    # 3.1. í† í¬ë‚˜ì´ì € ë° ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    base_model = AutoModel.from_pretrained(
        model_path, 
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    base_model.to(DEVICE)

    # 3.2. LoRA ì„¤ì • ë° ì ìš©
    lora_config = LoraConfig(
        r=LORA_R, 
        lora_alpha=LORA_ALPHA,
        target_modules=["query", "value", "key"], # XLM-R/RoBERTa ê¸°ë°˜ ëª¨ë¸ì˜ ì¼ë°˜ì ì¸ íƒ€ê²Ÿ
        lora_dropout=0.1,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
    )
    model = get_peft_model(base_model, lora_config)
    print("LoRA ëª¨ë¸ë¡œ ë³€í™˜ ì™„ë£Œ. í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°:")
    model.print_trainable_parameters() 

    # 3.3. Training Arguments ì„¤ì •
    training_args = HfTrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM_STEPS, # ğŸš¨ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì„¤ì •
        learning_rate=LEARNING_RATE,
        warmup_ratio=WARMUP_RATIO,
        lr_scheduler_type=SchedulerType.COSINE, 
        optim="adamw_torch", 
        fp16=True, 
        logging_steps=50,
        remove_unused_columns=False, # <-- ì´ ì¤„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

        # ğŸŒŸ Evaluation/Save Strategy ğŸŒŸ
        eval_strategy="steps",    
        eval_steps=EVAL_STEPS,          
        save_strategy="steps",
        save_steps=EVAL_STEPS,          
        save_total_limit=3,             
        load_best_model_at_end=True,    
        metric_for_best_model="eval_ndcg@10", # ğŸš¨ Metric ì´ë¦„ì„ 'eval_' ì ‘ë‘ì‚¬ì™€ í•¨ê»˜ ëª…ì‹œ
        greater_is_better=True,
    )

    # 3.4. Data Collator ë° Evaluator ì„¤ì •
    data_collator = CustomDPRCollator(tokenizer)
    
    # IR EvaluatorëŠ” Trainerì™€ ë¶„ë¦¬í•˜ì—¬ Callbacksìœ¼ë¡œ ì²˜ë¦¬
    # Note: transformers.TrainerëŠ” EvaluationStrategy.STEPSë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
    
    # 3.5. Custom Trainer ì¸ìŠ¤í„´ìŠ¤í™”
    trainer = CustomDPRTrainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset, # ê²€ì¦ ë°ì´í„°ì…‹ ì¶”ê°€
        tokenizer=tokenizer,
    )
    
    # 3.6. Callbacks ì¶”ê°€ (Early Stopping ë° IR Evaluator)
    from transformers import TrainerCallback
    
    # IR Evaluatorë¥¼ Callback í˜•íƒœë¡œ ë§Œë“¤ì–´ì¤˜ì•¼ í•¨ (Trainerì— ì§ì ‘ Evaluator ì¸ìê°€ ì—†ê¸° ë•Œë¬¸)
    class IREvaluatorCallback(TrainerCallback):
        def __init__(self, evaluator):
            self.evaluator = evaluator
        
        def on_step_end(self, args, state, control, **kwargs):
            if state.global_step > 0 and state.global_step % args.eval_steps == 0:
                print(f"\n--- Running IR Evaluation at step {state.global_step} ---")
                
                # ğŸš¨ S-T Evaluator ì‹¤í–‰ ë° ê²°ê³¼ ì–»ê¸°
                output_scores = self.evaluator(model.module if hasattr(model, 'module') else model)
                
                # NDCG@10ì„ Trainerê°€ ì¸ì‹í•˜ë„ë¡ ë¡œê·¸ì— ì¶”ê°€
                ndcg_score = output_scores.get('ndcg@10')
                if ndcg_score is not None:
                    state.log_history.append({
                        'global_step': state.global_step,
                        'eval_ndcg@10': ndcg_score
                    })
                
                # Early Stoppingì„ ìœ„í•œ ì»¨íŠ¸ë¡¤ ë°˜í™˜ (ë³µì¡í•˜ë¯€ë¡œ ì¼ë‹¨ ë¡œê·¸ë§Œ ì‚¬ìš©)
                return control 
            
    trainer.add_callback(IREvaluatorCallback(
        InformationRetrievalEvaluator(
            queries, corpus, relevant_docs, name='ndcg@10', main_score_function=SimilarityFunction.COSINE
        )
    ))

    return trainer


# --- 4. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == '__main__':
    try:
        # 1. ë°ì´í„° ì¤€ë¹„
        train_data, eval_data, val_queries, val_corpus, val_qrels = load_and_prepare_data(DATA_FILE_PATH)
        
        if not train_data:
            print("FATAL ERROR: Training data is empty. Exiting.")
            exit()
            
        # 2. íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer = setup_lora_trainer(train_data, eval_data, val_queries, val_corpus, val_qrels, LOCAL_MODEL_PATH)
        
        # 3. í›ˆë ¨ ì‹œì‘
        print("\n" + "="*50)
        print("Starting BGE-M3 LoRA Fine-tuning (Custom Trainer Mode)...")
        print("="*50)
        
        trainer.train()
        
        # 4. ê°€ì¤‘ì¹˜ ì €ì¥ (PEFT ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥)
        final_save_path = os.path.join(OUTPUT_DIR, "final_best_lora_model")
        
        # load_best_model_at_end=Trueì— ë”°ë¼ ìµœì  ëª¨ë¸ì´ ë¡œë“œëœ ìƒíƒœì—ì„œ ì €ì¥
        trainer.save_model(final_save_path) 
        
        # ğŸš¨ LoRA ëª¨ë¸ë§Œ ì €ì¥ (ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½)
        # model.save_pretrained(final_save_path) # Trainerê°€ ì²˜ë¦¬í•˜ë¯€ë¡œ ë¶ˆí•„ìš”
        
        print(f"\nâœ“ Fine-tuning complete. Best LoRA weights saved to {final_save_path}")

    except Exception as e:
        print(f"\n\nğŸš¨ CRITICAL ERROR DURING TRAINING: {e}")
        traceback.print_exc()
        print("Please check file paths, GPU memory, and data keys.")